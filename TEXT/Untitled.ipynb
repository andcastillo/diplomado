{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de palabras con tweeter\n",
    "\n",
    "Ahora usermos nuestro conjunto de entrenamiento de tweeter para construir un diccionario de verdad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 05:15:25,215 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/var/folders/kr/pp942k8n6_ncb5h3nm_27_780000gn/T\" will be used to save temporary dictionary and corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 05:15:25,471 : INFO : built Dictionary(2352 unique tokens: ['1', 'the', 'da', 'vinci', 'code']...) from 7086 documents (total 84117 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(103 unique tokens: ['da', 'vinci', 'code', 'book', 'just']...)\n",
      "{'da': 0, 'vinci': 1, 'code': 2, 'book': 3, 'just': 4, 'awesome': 5, 'this': 6, 'was': 7, 'first': 8, 'read': 9, 'but': 10, 'like': 11, 'more': 12, 'i': 13, 'liked': 14, 'it': 15, \"it's\": 16, 'not': 17, 'an': 18, 'at': 19, 'we': 20, 'went': 21, 'which': 22, 'loved': 23, 'want': 24, 'thought': 25, 'with': 26, 'good': 27, 'movie': 28, 'one': 29, 'most': 30, 'beautiful': 31, 'movies': 32, 'me': 33, 'then': 34, 'on': 35, 'my': 36, 'really': 37, 'love': 38, 'also': 39, 'would': 40, 'be': 41, '\"': 42, 'people': 43, 'being': 44, 'too': 45, 'so': 46, 'into': 47, 'that': 48, 'by': 49, 'way': 50, 'if': 51, 'you': 52, 'as': 53, 'reading': 54, 'because': 55, 'much': 56, 'how': 57, 'oh': 58, 'hate': 59, 'out': 60, 'fucking': 61, 'are': 62, 'right': 63, 'am': 64, 'only': 65, 'up': 66, 'think': 67, 'about': 68, 'from': 69, '3': 70, 'saw': 71, 'know': 72, 'who': 73, 'absolutely': 74, '2': 75, 'tom': 76, 'have': 77, 'series': 78, 'mission': 79, 'impossible': 80, 'ok': 81, 'can': 82, 'story': 83, '/': 84, 'guy': 85, 'why': 86, 'down': 87, 'or': 88, 'harry': 89, 'potter': 90, 'sucked': 91, 'start': 92, 'here': 93, 'brokeback': 94, 'mountain': 95, '0': 96, 'terrible': 97, 'sucks': 98, 'suck': 99, 'stupid': 100, 'horrible': 101, 'depressing': 102}\n",
      "<__main__.MyCorpus object at 0x104909518>\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "from gensim import corpora\n",
    "from six import iteritems\n",
    "\n",
    "\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().replace(\".\",\"\").replace(\",\",\"\").replace(\": \",\"\").replace(\"!\",\"\").split() for line in open('datasets/train.txt'))\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in 1 is'.split())\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq < 100]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'twiter.dict'))\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('datasets/train.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "            \n",
    "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
