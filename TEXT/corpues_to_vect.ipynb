{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming – Un documento a la vez \n",
    "\n",
    "Note que el corpus anterior estaba en la memoria completamente como una lista plana de Python. Esto no siempre es posible. Imaginemos que tenemos millones de documentos, y que no caben en la memoria. Entonces debemos ser poder trabajar con datos que están almacenados en disco duro directamente, y procesar línea por línea de ellos ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 05:03:18,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-11-04 05:03:18,491 : INFO : built Dictionary(42 unique tokens: ['human', 'machine', 'interface', 'for', 'lab']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/var/folders/kr/pp942k8n6_ncb5h3nm_27_780000gn/T\" will be used to save temporary dictionary and corpus.\n",
      "<__main__.MyCorpus object at 0x107b06198>\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(6, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n",
      "[(2, 1), (11, 1), (13, 1), (16, 1), (17, 1), (18, 1)]\n",
      "[(0, 1), (10, 1), (13, 2), (17, 1), (19, 1), (20, 1), (21, 1)]\n",
      "[(10, 1), (11, 1), (14, 1), (15, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]\n",
      "[(10, 1), (16, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(10, 1), (16, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)]\n",
      "[(10, 1), (19, 1), (31, 1), (33, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]\n",
      "[(8, 1), (9, 1), (33, 1), (36, 1)]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "from gensim import corpora\n",
    "\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('datasets/mycorpus.txt'))\n",
    "#dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('datasets/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "            \n",
    "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando nuestro diccionario\n",
    "We are going to create the dictionary from the mycorpus.txt file without loading the entire file into memory. Then, we will generate the list of token ids to remove from this dictionary by querying the dictionary for the token ids of the stop words, and by querying the document frequencies dictionary (dictionary.dfs) for token ids that only appear once. Finally, we will filter these token ids out of our dictionary. Keep in mind that dictionary.filter_tokens (and some other functions such as dictionary.add_document) will call dictionary.compactify() to remove the gaps in the token id series thus enumeration of remaining tokens can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 1, 2: 2, 3: 1, 4: 1, 5: 1, 6: 2, 7: 1, 8: 2, 9: 2, 10: 6, 11: 3, 12: 1, 13: 3, 14: 2, 15: 2, 16: 3, 17: 2, 18: 1, 19: 2, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 3, 32: 1, 33: 3, 34: 1, 35: 1, 36: 2, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1}\n",
      "Dictionary(12 unique tokens: ['human', 'interface', 'computer', 'survey', 'user']...)\n",
      "{0: 2, 1: 2, 2: 2, 3: 2, 4: 3, 5: 3, 6: 2, 7: 2, 8: 2, 9: 3, 10: 3, 11: 2}\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "print(dictionary)\n",
    "print(dictionary.dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
